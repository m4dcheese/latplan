{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     GaussianNoise-1            [-1, 3, 64, 64]               0\n",
      "            Conv2d-2           [-1, 64, 64, 64]           4,864\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4           [-1, 64, 64, 64]         102,464\n",
      "              ReLU-5           [-1, 64, 64, 64]               0\n",
      "            Conv2d-6           [-1, 64, 64, 64]         102,464\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]         102,464\n",
      "              ReLU-9           [-1, 64, 64, 64]               0\n",
      "SlotAttention_encoder-10           [-1, 64, 64, 64]               0\n",
      "           Linear-11           [-1, 64, 64, 64]             320\n",
      "SoftPositionEmbed-12           [-1, 64, 64, 64]               0\n",
      "        LayerNorm-13             [-1, 4096, 64]             128\n",
      "           Linear-14             [-1, 4096, 64]           4,160\n",
      "             ReLU-15             [-1, 4096, 64]               0\n",
      "           Linear-16             [-1, 4096, 64]           4,160\n",
      "              MLP-17             [-1, 4096, 64]               0\n",
      "        LayerNorm-18             [-1, 4096, 64]             128\n",
      "           Linear-19             [-1, 4096, 64]           4,160\n",
      "           Linear-20             [-1, 4096, 64]           4,160\n",
      "        LayerNorm-21               [-1, 10, 64]             128\n",
      "           Linear-22               [-1, 10, 64]           4,160\n",
      "          GRUCell-23                   [-1, 64]               0\n",
      "        LayerNorm-24               [-1, 10, 64]             128\n",
      "           Linear-25              [-1, 10, 128]           8,320\n",
      "             ReLU-26              [-1, 10, 128]               0\n",
      "           Linear-27               [-1, 10, 64]           8,256\n",
      "        LayerNorm-28               [-1, 10, 64]             128\n",
      "           Linear-29               [-1, 10, 64]           4,160\n",
      "          GRUCell-30                   [-1, 64]               0\n",
      "        LayerNorm-31               [-1, 10, 64]             128\n",
      "           Linear-32              [-1, 10, 128]           8,320\n",
      "             ReLU-33              [-1, 10, 128]               0\n",
      "           Linear-34               [-1, 10, 64]           8,256\n",
      "        LayerNorm-35               [-1, 10, 64]             128\n",
      "           Linear-36               [-1, 10, 64]           4,160\n",
      "          GRUCell-37                   [-1, 64]               0\n",
      "        LayerNorm-38               [-1, 10, 64]             128\n",
      "           Linear-39              [-1, 10, 128]           8,320\n",
      "             ReLU-40              [-1, 10, 128]               0\n",
      "           Linear-41               [-1, 10, 64]           8,256\n",
      "        LayerNorm-42               [-1, 10, 64]             128\n",
      "           Linear-43               [-1, 10, 64]           4,160\n",
      "          GRUCell-44                   [-1, 64]               0\n",
      "        LayerNorm-45               [-1, 10, 64]             128\n",
      "           Linear-46              [-1, 10, 128]           8,320\n",
      "             ReLU-47              [-1, 10, 128]               0\n",
      "           Linear-48               [-1, 10, 64]           8,256\n",
      "    SlotAttention-49               [-1, 10, 64]               0\n",
      "           Linear-50                 [-1, 1000]         641,000\n",
      "             Tanh-51                 [-1, 1000]               0\n",
      "           Linear-52                  [-1, 480]         480,480\n",
      "    GumbelSoftmax-53                  [-1, 480]               0\n",
      "           Linear-54                 [-1, 1000]         481,000\n",
      "             Tanh-55                 [-1, 1000]               0\n",
      "           Linear-56                  [-1, 640]         640,640\n",
      "           Linear-57             [-1, 8, 8, 64]             320\n",
      "SoftPositionEmbed-58             [-1, 64, 8, 8]               0\n",
      "  ConvTranspose2d-59           [-1, 64, 16, 16]         102,464\n",
      "             ReLU-60           [-1, 64, 16, 16]               0\n",
      "  ConvTranspose2d-61           [-1, 64, 32, 32]         102,464\n",
      "             ReLU-62           [-1, 64, 32, 32]               0\n",
      "  ConvTranspose2d-63           [-1, 64, 64, 64]         102,464\n",
      "             ReLU-64           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-65           [-1, 64, 64, 64]         102,464\n",
      "             ReLU-66           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-67            [-1, 4, 64, 64]           2,308\n",
      "SlotAttention_decoder-68            [-1, 4, 64, 64]               0\n",
      "          Softmax-69        [-1, 10, 1, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 3,069,044\n",
      "Trainable params: 3,069,044\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 48.17\n",
      "Params size (MB): 11.71\n",
      "Estimated Total Size (MB): 59.93\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Global Step 0 Train Loss: 0.168783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:12,  3.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10205/559674276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_slot_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/thesis-latplan/state_ae/train_slot_attention.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mrtpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis-latplan/state_ae/train_slot_attention.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(net, loader, optimizer, criterion, scheduler, writer, parameters, epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0miters_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cuda:{parameters.device_ids[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cuda:{parameters.device_ids[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train_slot_attention import train\n",
    "\n",
    "net = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(net, (1, 84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from data import get_loader\n",
    "import torch\n",
    "import numpy as np\n",
    "loader = get_loader(total_samples=1)\n",
    "iter, (x, mask) = list(enumerate(loader))[0]\n",
    "x = x.to(torch.device(\"cuda\"))\n",
    "net.train()\n",
    "recon_combined, recons, masks, slots = net.forward(x)\n",
    "\n",
    "from util import show_mnist_images\n",
    "\n",
    "show_mnist_images(x.cpu().detach().numpy()[0])\n",
    "show_mnist_images(recon_combined.cpu().detach().numpy()[0])\n",
    "show_mnist_images(recons[0].cpu().detach().numpy()[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e926bc823418c448474a8974b2c2aa4c0f7ebfb59bb98328b5791be7eb5e18cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('latplan': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
